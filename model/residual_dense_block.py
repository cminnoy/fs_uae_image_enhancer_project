import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple
from activations import get_activation 
from triplet_attention import TripletAttention

class ResidualDenseBlock(nn.Module):
    """
    Residual Dense Block (RDB) as commonly used in image restoration tasks (e.g., ESRGAN).
    It features multiple convolutional layers with dense connections, local residual learning,
    and supports custom activation functions.

    The block consists of 'n_convs' (default 3) convolutional layers.
    Each convolutional layer is followed by two user-defined activation functions.
    Dense feature fusion concatenates the outputs of all internal convolutional layers.
    A skip connection is added from the input of the RDB to its final output.

    Attributes:
        in_channels (int): Number of input channels.
        out_channels (int): Number of output channels.
        growth_rate (int): How many channels each layer in the RDB adds to the feature map.
        n_convs (int): Number of convolutional layers within the RDB.
        activation_names (Tuple[str, str]): A tuple of two strings, specifying the names
                                            of the activation functions for each slot.
                                            These names must be registered in the
                                            ACTIVATION_REGISTRY from activations.py.
        activation_params (Optional[Tuple[Dict[str, Any], Dict[str, Any]]]): Optional
                                            tuple of dictionaries, where each dictionary
                                            contains parameters for the corresponding
                                            activation function's constructor.
    """
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        growth_rate: int,
        n_convs: int = 3,
        activation_names: Tuple[str, str] = ('relu', 'identity'),
        activation_params: Optional[Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]] = (None, None),
        use_triplet_attention: bool = False,
        use_local_residual_learning: bool = True
    ):
        """
        Initializes the ResidualDenseBlock.

        Args:
            in_channels (int): Number of input channels for the block.
            out_channels (int): Number of output channels for the block. This is the
                                 number of channels after the 1x1 convolution for DFF.
            growth_rate (int): Growth rate of the dense block (number of new features
                               generated by each convolutional layer).
            n_convs (int): Number of 3x3 convolutional layers in the dense block. Default is 3.
            activation_names (Tuple[str, str]): Names of the two activation functions for each
                                                convolutional layer. E.g., ('relu', 'identity').
            activation_params (Optional[Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]]):
                                                Optional parameters for the activation functions.
                                                Each element in the tuple corresponds to a
                                                dictionary of parameters for get_activation.
            use_triplet_attention (bool): Whether to apply triplet attention to the output
                                          of the dense block before the final 1x1 convolution.
                                          Default is False.
        """
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.growth_rate = growth_rate
        self.n_convs = n_convs
        self.activation_names = activation_names
        self.activation_params = activation_params
        self.use_triplet_attention = use_triplet_attention
        if self.use_triplet_attention:
            self.triplet_attention = TripletAttention(no_spatial=False)
            
        if len(activation_names) != 2 or (activation_params is not None and len(activation_params) != 2):
            raise ValueError("activation_names and activation_params must be tuples of length 2.")

        layers = []
        current_in_channels = in_channels
        for i in range(n_convs):
            # Convolutional layer
            conv_layer = nn.Conv2d(
                in_channels=current_in_channels,
                out_channels=growth_rate,
                kernel_size=3,
                padding=1,
                bias=True
            )
            layers.append(conv_layer)

            # Prepare activation parameters for this level
            act_params = []
            for j in range(2):
                params = None
                if activation_params and activation_params[j]:
                    params = activation_params[j].copy()
                    if "num_parameters" in params and isinstance(params["num_parameters"], str):
                        if params["num_parameters"] == "channel":
                            params["num_parameters"] = growth_rate
                        elif params["num_parameters"] == "global":
                            params["num_parameters"] = 1
                act_params.append(params)

            # First activation function slot
            act1 = get_activation(
                activation_names[0],
                params=act_params[0]
            )
            layers.append(act1)

            # Second activation function slot
            act2 = get_activation(
                activation_names[1],
                params=act_params[1]
            )
            layers.append(act2)

            current_in_channels += growth_rate

        self.dense_layers = nn.Sequential(*layers)

        self.dense_feature_fusion = nn.Conv2d(
            in_channels=current_in_channels,
            out_channels=out_channels,
            kernel_size=1,
            padding=0,
            bias=True
        )

        if use_local_residual_learning:
            # If local residual learning is enabled, we need to ensure the input and output channels match
            if in_channels != out_channels:
                self.local_residual_adjustment = nn.Conv2d(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    kernel_size=1,
                    padding=0,
                    bias=True
                )
            else:
                self.local_residual_adjustment = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the ResidualDenseBlock.

        Args:
            x (torch.Tensor): Input tensor to the RDB.

        Returns:
            torch.Tensor: Output tensor of the RDB.
        """
        initial_input = x
        feature_maps = [x] # Start with the input to the block for dense connections

        # Iterate through the dense layers
        # Each group of 3 layers (Conv, Act1, Act2) acts as one dense layer unit
        for i in range(self.n_convs):
            conv_idx = i * 3 # Index of the Conv2d layer
            current_input = torch.cat(feature_maps, dim=1) # Concatenate all previous feature maps
            
            # Pass through Conv2d, Act1, Act2
            output = self.dense_layers[conv_idx](current_input)
            output = self.dense_layers[conv_idx + 1](output) # First activation
            output = self.dense_layers[conv_idx + 2](output) # Second activation
            
            feature_maps.append(output) # Add current output to feature_maps for next concatenation

        # Concatenate all feature maps from the dense block for DFF
        concatenated_features = torch.cat(feature_maps, dim=1)

        # Apply triplet attention if enabled
        if self.use_triplet_attention:
            concatenated_features = self.triplet_attention(concatenated_features)

        # Apply Dense Feature Fusion (1x1 convolution)
        output = self.dense_feature_fusion(concatenated_features)

        # Local Residual Learning: Add the DFF output to the initial input (skip connection)
        if self.local_residual_adjustment is not None:
            initial_input = self.local_residual_adjustment(initial_input)
            output += initial_input

        return output
